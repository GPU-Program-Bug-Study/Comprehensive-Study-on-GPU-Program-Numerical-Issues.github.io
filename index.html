<!DOCTYPE html>
<html>
<style>
table, th, td {
  border: 1px solid black;
  border-collapse: collapse;
  border-color: #96D4D4
}
</style>
    <head>
        <h1>A Comprehensive Study on Numerical Issues in GPU Programs</h1>
    </head>
    
<h2 id="tables">Tables</h2>
<p id="tableA">Table A shows the total GitHub repositories included in the study after performing the manual inspection</p>
<label   for="tableA">Table A: Summary of GitHub Issues</label>
<table  style="width:40%">
    <thead><tr><th title="Field #1">GitHub Project</th>
    <th title="Field #2">Stars</th>
    <th title="Field #3">Commits</th>
    <th title="Field #4">Contributors</th>
    <th title="Field #5">Issues Included</th>
    </tr></thead>
    <tbody><tr>
    <td>CuPy</td>
    <td align="right">6.3K</td>
    <td align="right">25799</td>
    <td align="right">263</td>
    <td align="right">62</td>
    </tr>
    <tr>
    <td>TensorFlow</td>
    <td align="right">168K</td>
    <td align="right">135063</td>
    <td align="right">3197</td>
    <td align="right">42</td>
    </tr>
    <tr>
    <td>PyTorch</td>
    <td align="right">58.6K</td>
    <td align="right">51541</td>
    <td align="right">2430</td>
    <td align="right">30</td>
    </tr>
    <tr>
    <td>cuDF</td>
    <td align="right">5K</td>
    <td align="right">35572</td>
    <td align="right">194</td>
    <td align="right">15</td>
    </tr>
    <tr>
    <td>Numba</td>
    <td align="right">7.8K</td>
    <td align="right">23157</td>
    <td align="right">272</td>
    <td align="right">14</td>
    </tr>
    <tr>
    <td>Tranformers</td>
    <td align="right">70.1K</td>
    <td align="right">10611</td>
    <td align="right">1392</td>
    <td align="right">10</td>
    </tr>
    <tr>
    <tr>
    <td>Ginkgo</td>
    <td align="right">234</td>
    <td align="right">5459</td>
    <td align="right">22</td>
    <td align="right">5</td>
    </tr>
    <tr>
    <td>cuML</td>
    <td align="right">2.9K</td>
    <td align="right">14862</td>
    <td align="right">116</td>
    <td align="right">5</td>
    </tr>
    <tr>
    <td>TensorRT</td>
    <td align="right">5.9K</td>
    <td align="right">415</td>
    <td align="right">48</td>
    <td align="right">5</td>
    </tr>
    <tr>
    <td>Apache-incubator-mxnet</td>
    <td align="right">20.1K</td>
    <td align="right">11890</td>
    <td align="right">874</td>
    <td align="right">4</td>
    </tr>
    <tr>
    <td>mmdetection</td>
    <td align="right">21.3K</td>
    <td align="right">2115</td>
    <td align="right">356</td>
    <td align="right">4</td>
    </tr>
    <tr>
    <td>PyTorch-Lightning</td>
      <td align="right">20K</td>
     <td align="right">7597</td>
    <td align="right">744</td>
    <td align="right">4</td>
    </tr>
    <tr>
    <td>MatX</td>
    <td align="right">549</td>
    <td align="right">270</td>
    <td align="right">15</td>
    <td align="right">4</td>
    </tr>
    <tr>
    <td>Paddle</td>
    <td align="right">18.8K</td>
     <td align="right">37448</td>
    <td align="right">615</td>
    <td align="right">3</td>
    </tr>
    <tr>
    <td>Cutlass</td>
    <td align="right">2.1K</td>
    <td align="right">266</td>
    <td align="right">51</td>
    <td align="right">3</td>
    </tr>
    <tr>
    <td>GPUweb</td>
    <td align="right">3.2K</td>
    <td align="right">1786</td>
    <td align="right">83</td>
    <td align="right">3</td>
    </tr>
    <tr>
    <td>Onnx</td>
    <td align="right">13.2K</td>
    <td align="right">2044</td>
    <td align="right">237</td>
    <td align="right">3</td>
    </tr>
    <tr>
    <td>xgboost</td>
    <td align="right">23.2K</td>
    <td align="right">5923</td>
    <td align="right">554</td>
    <td align="right">3</td>
    </tr>
    <tr>
    <td>pyVista</td>
    <td align="right">1.4K</td>
    <td align="right">3001</td>
    <td align="right">113</td>
    <td align="right">2</td>
    </tr>
    <tr>
    <td>cuGraph</td>
    <td align="right">1.1K</td>
    <td align="right">5470</td>
    <td align="right">74</td>
    <td align="right">2</td>
    </tr>
    <tr>
    <td>CUDA.jl</td>
    <td align="right">830</td>
    <td align="right">7244</td>
    <td align="right">113</td>
    <td align="right">2</td>
    </tr>
    <tr>
    <td>pymc-dev</td>
    <td align="right">7K</td>
    <td align="right">8892</td>
    <td align="right">371</td>
    <td align="right">2</td>
    </tr>
    <tr>
    <td>google-autoML</td>
    <td align="right">5.1K</td>
    <td align="right">685</td>
    <td align="right">34</td>
    <td align="right">2</td>
    </tr>
    <tr>
    <td>CLIMA-Oceananigans.jl</td>
    <td align="right">707</td>
    <td align="right">10595</td>
    <td align="right">41</td>
    <td align="right">2</td>
    </tr>
    <tr>
    <td>ray</td>
    <td align="right">22K</td>
    <td align="right">14191</td>
    <td align="right">722</td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>napari</td>
    <td align="right">1.5K</td>
    <td align="right">2474</td>
    <td align="right">128</td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>jiesutd-NCRFPP</td>
    <td align="right">1.8K</td>
    <td align="right">127</td>
    <td align="right">9</td>
    <td align="right">1</td>
    </tr>
    <tr>
    <td>AMDMIGraphX</td>
    <td align="right">92</td>
    <td align="right">4410</td>
    <td align="right">28</td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>Thrust</td>
    <td align="right">4.1K</td>
    <td align="right">4578</td>
    <td align="right">74</td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>GPytorch</td>
    <td align="right">2.9K</td>
    <td align="right">3683</td>
    <td align="right">86</td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>Stumpy</td>
    <td align="right">2.4K</td>
    <td align="right">1120</td>
    <td align="right">27</td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>Spark-rapids</td>
    <td align="right">450</td>
    <td align="right">4587</td>
    <td align="right">59</td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>UPIT</td>
    <td align="right">94</td>
    <td align="right">165</td>
    <td align="right">5</td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>DeepSpeed</td>
    <td align="right">7.8K</td>
    <td align="right">1116</td>
    <td align="right">137</td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>CSAROfeen</td>
    <td align="right">16</td>
    <td align="right">52129</td>
    <td align="right">1</td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>OpenCV</td>
    <td align="right">63.7K</td>
    <td align="right">32140</td>
    <td align="right">1404</td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>catboost</td>
    <td align="right">6.7K</td>
    <td align="right">23866</td>
    <td align="right">293</td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>NX</td>
    <td align="right">1.9K</td>
    <td align="right">1410</td>
    <td align="right">57</td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>Onnxruntime</td>
    <td align="right">7.4K</td>
    <td align="right">7334</td>
    <td align="right">420</td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>mrdoob-three.js</td>
    <td align="right">85.2K</td>
    <td align="right">40515</td>
    <td align="right">1672</td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>AMDVLK</td>
    <td align="right">1.3K</td>
    <td align="right">119</td>
    <td align="right">9</td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>WGPU</td>
    <td align="right">5.6K</td>
    <td align="right">3849</td>
    <td align="right">260</td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>FARM</td>
    <td align="right">1.6K</td>
    <td align="right">594</td>
    <td align="right">110</td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>hoomd-blue</td>
    <td align="right">215</td>
    <td align="right">21504</td>
    <td align="right">79</td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>blazingsql</td>
    <td align="right">1.8K</td>
    <td align="right">8208</td>
    <td align="right">33</td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>awslabs-djl</td>
    <td align="right">2.7K</td>
    <td align="right">3383</td>
    <td align="right">73</td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>MIOpen</td>
    <td align="right">720</td>
    <td align="right">8925</td>
    <td align="right">77</td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>Kitty</td>
    <td align="right">15.9K</td>
    <td align="right">10546</td>
    <td align="right">199</td>
    <td align="right">1</td>
    </tr>
     <tr>
    <td>faiss</td>
    <td align="right">17.9K</td>
    <td align="right">654</td>
    <td align="right">98</td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>Charm</td>
    <td align="right">147</td>
    <td align="right">24927</td>
    <td align="right">82</td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>GPFlow</td>
    <td align="right">1.7K</td>
    <td align="right">2390</td>
    <td align="right">72</td>
    <td align="right">1</td>
    </tr>
     <tr>
    <td>OpenNMT</td>
    <td align="right">5.7K</td>
    <td align="right">2598</td>
    <td align="right">164</td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>glslang</td>
    <td align="right">2.3K</td>
    <td align="right">4586</td>
    <td align="right">238</td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>maskrcnn-benchmark</td>
    <td align="right">2</td>
    <td align="right">252</td>
    <td align="right">1</td>
    <td align="right">1</td>
    </tr>
    <tr>
    <td>futhark</td>
    <td align="right">1.8K</td>
    <td align="right">11205</td>
    <td align="right">62</td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>OMEinsum.jl</td>
    <td align="right">132</td>
    <td align="right">169</td>
    <td align="right">7</td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>facebookresearch-meshrcnn</td>
    <td align="right">1K</td>
    <td align="right">21</td>
    <td align="right">7</td>
    <td align="right">1</td>
    </tr>
     <tr>
    <td>uber-ludwig</td>
    <td align="right">8.5K</td>
    <td align="right">2586</td>
    <td align="right">126</td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>tensorpack</td>
    <td align="right">6.2K</td>
    <td align="right">2939</td>
    <td align="right">56</td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>spaCy</td>
    <td align="right">24.2K</td>
    <td align="right">15598</td>
    <td align="right">626</td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>perses</td>
    <td align="right">107</td>
    <td align="right">3797</td>
    <td align="right">14</td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>linux</td>
    <td align="right">216</td>
    <td align="right">983155</td>
    <td align="right">1</td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>verification-classifier</td>
    <td align="right">8</td>
    <td align="right">911</td>
    <td align="right">9</td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>pointcloud</td>
    <td align="right">7.7K</td>
    <td align="right">13932</td>
    <td align="right">471</td>
    <td align="right">1</td>
    </tr>
    <tr>
    <td>darknet</td>
    <td align="right">19.7K</td>
    <td align="right">2222</td>
    <td align="right">1</td>
    <td align="right">1</td>
    </tr>
</tbody></table>
<br/>
<br/>
    
    
    
    <body> 
        <head>
           <h2> Floating Point Support in GPU Architecture </h2>
        </head>
        
        <p style="color:black;font-size:20px;"> GeForce 256 was the first chip to combine the vertex computations for transformation and lighting and the fragment computations on the same chip[1]. 
             This fully integrated graphics engine was named a ‚Äúgraphics processing unit‚Äù or GPU. 
             Offloading the vertex computations from the host enabled higher geometric complexity in games at the cost of requiring significant floating-point performance.
             For example, the perspective transformation requires a 4X4 matrix‚Äìvector multiply and a perspective division operation. </p>
        
        <p style="color:black;font-size:20px;"> GeForce 6 had a peak performance of 108 billion single-precision floating-point operations per second (108 GFLOPS) [1]. 
               However, programming these GPUs was challenging because the input and output were very restricted. 
               Fragment shaders only received input from interpolated vertex attributes and textures and only deposited output into the frame buffer. 
               Programmers had to cast their applications as rendering texture mapped and blended triangles to harness the FLOPS of these GPUs[1]. </p>
        
        <p style="color:black;font-size:20px;"> The stream processing model influenced the design of GPUs intended for computing [1]. 
               The GeForce 8 GPU or G80 introduced streaming multiprocessors (SMs) that were used to run both vertex and fragment shaders.
               These SMs could also run ‚Äúcompute shaders‚Äù independent of the graphics pipeline specifically in the form of CUDA cooperative thread arrays. 
               A shared memory facilitated communication between the threads in an array. 
               The G80 ‚ÄúTesla‚Äù GPU and CUDA reduced the barriers to developing GPU-based scientific and other general-purpose applications[1]. 
               G80 was a massively parallel threaded processor, and CUDA was a thread- and data-parallel C-like programming language. </p>
        
        <p style="color:black;font-size:20px;"> The development of Fermi generation of GPUs helps to address the floating point issues in GPUs [1][2]. 
         Single precision floating point instructions  support subnormal numbers by default in hardware. 
         Subnormal numbers are small numbers that lie between zero and the smallest normalized number of a given floating point number system.
         Fermi‚Äôs floating point units handle subnormal numbers in hardware, allowing values to gradually underflow to zero with no performance penalty. 
         Additionally, Fermi GPUs add support to double precision computation. </p>
          
    <figure>
    <img src= "../Images/fermi.JPG" alt= "Fermi">
    <figcaption>Comparison of Fermi with previous GPUs[2]</figcaption>
    </figure>
        
         
        <p style="color:black;font-size:20px;"> With the development of Kepler GPUs, the support for atomic memory operations helped parallel programming to gain massive performance benefits[3]. 
            Throughput of global memory atomic operations on Kepler GK110/210 are substantially improved compared to the Fermi generation. 
            Atomic operation throughput to a common global memory address is improved by 9x to one operation per clock. 
            Atomic operation throughput to independent global addresses is also significantly accelerated, and logic to handle address conflicts has been made more efficient. 
            Atomic operations can often be processed at rates similar to global load operations. 
             This speed increase makes atomics fast enough to use frequently within kernel inner loops, eliminating the separate reduction passes that were previously required by some algorithms to consolidate results. 
             Kepler GK110 also expands the native support for 64-bit atomic operations in global memory. 
             In addition to atomicAdd, atomicCAS, and atomicExch (which were also supported by Fermi and Kepler GK104), GK110 supports the following:  atomicMin,  atomicMax,  atomicAnd,  atomicOr,  atomicXor.
            Other atomic operations which are not supported natively (for example 64-bit floating point atomics) may be emulated using the compare-and-swap (CAS) instruction. </p>
        
        <p style="color:black;font-size:20px;"> According to Dally et al. [1], the next level of support is a set of numerical libraries including CuBLAS, CuSparse, and CuFFT that provide highly optimized code for the key functions of many numerical programs. 
         Applications can then leverage these libraries to exploit GPU capabilities. 
         Today, over 600 HPC applications are accelerated by GPUs,9 including molecular dynamics codes such as GROMACS, NAMD, AMBER, and LAMMPS; weather codes such as WRF; fluid dynamics codes such as ANSYS and OpenFOAM; 
        chemistry codes such as Gaussian, VASP, Quantum Espresso, and GAMESS; and structural analysis codes such as LS-DYNA and ANSYS. </p>


       <p style="color:black;font-size:20px;"> Pascal GPUs  provide superior scheduling and overlapped load/store instructions to increase floating point utilization[4]. 
        New innovations in our Pascal architecture, including native 16-bit floating point (FP) precision, allow GP100 to deliver great speedups for many Deep Learning algorithms. 
        These algorithms do not require high levels of floating-point precision, but they gain large benefits from the additional computational power FP16 affords, and the reduced storage requirements for 16-bit datatypes. 
        Storing FP16 data compared to higher precision FP32 or FP64 reduces memory usage of the neural network and thus allows training and deploying of larger networks. 
        Using FP16 computation improves performance up to 2x compared to FP32 arithmetic, and similarly FP16 data transfers take less time than FP32 or FP64 transfers.
        Also, The atomic addition operation in global memory has been extended to include FP64 data. 
        The atomicAdd() function in CUDA now applies to 32 and 64-bit integer and floating-point data. 
        The rounding mode for floating-point is round-to-nearest-even for all floating-point atomic add operations (formerly, FP32 atomic addition used round-to-zero).</p>
   
     <figure>
    <img src= "../Images/pascal.JPG" alt= "Pascal">
    <figcaption>Tesla P100 Compared to Prior Generation Tesla products [4]</figcaption>
    </figure>
          
      <p style="color:black;font-size:20px;">  Tesla V100 GPUs deliver industry-leading floating-point and integer performance [5].
       The Tesla V100 GPU contains 640 Tensor Cores: eight (8) per SM and two (2) per each processing block (partition) within an SM. 
      In Volta GV100, each Tensor Core performs 64 floating point FMA operations per clock, and eight Tensor Cores in an SM perform a total of 512 FMA operations (or 1024 individual floating point operations) per clock.
      For deep learning inference, V100 Tensor Cores provide up to 6x higher peak TFLOPS compared to standard FP16 operations on P100. 
      Tensor Cores and their associated data paths are custom-designed to dramatically increase floating-point compute throughput with high energy efficiency. </p>
      
    <figure>
    <img src= "../Images/volta.JPG" alt= "Volta">
    <figcaption>Comparison of NVIDIA Tesla GPUs [5]</figcaption>
    </figure>
        
      <p style="color:black;font-size:20px;"> A Turing 102 GPU contains 576 Tensor Cores: eight per SM and two per each processing block within an SM [6]. 
      Each Tensor Core can perform up to 64 floating point fused multiply-add (FMA) operations per clock using FP16 inputs. 
      Eight Tensor Cores in an SM perform a total of 512 FP16 multiply and accumulate operations per clock, or 1024 total FP operations per clock. 
      The new INT8 precision mode works at double this rate, or 2048 integer operations per clock. 
       Turing Tensor Cores provide significant speedups to matrix operations and are used for both deep learning training and inference operations in addition to new neural graphics functions. </p>
     
      <figure>
      <img src= "../Images/Turing1.JPG" alt= "Turing">
      </figure>
        
     <figure>
    <img src= "../Images/Turing2.JPG" alt= "Turing">
    <figcaption>Comparison of NVIDIA Pascal GP104 to Turing TU106 GPUs [6]</figcaption>
    </figure>
        
       <p style="color:black;font-size:20px;"> Ampere GPUs provide TensorFloat support to GPU architecture [1] [7]. 
       New TensorFloat-32 (TF32) Tensor Core operations in A100 provide an easy path to accelerate FP32 input/output data in DL frameworks and HPC, running 10x faster than V100 FP32 FMA operations or 20x faster with sparsity.  
       The Tensor Core architecture also supports high-throughput computation for different numerical representations, including binary (INT1), INT4, INT8, FP16, and BFloat16 (8-bit exponent, 7-bit mantissa). 
       For FP16/FP32 mixed-precision DL, the A100 Tensor Core delivers 2.5x the performance of V100, increasing to 5x with sparsity. 
       New Bfloat16 (BF16)/FP32 mixed-precision Tensor Core operations run at the same rate as FP16/FP32 mixed-precision. 
       Tensor Core acceleration of INT8, INT4, and binary round out support for DL inferencing, with A100 sparse INT8 running 20x faster than V100 INT8. 
       For HPC, the A100 Tensor Core includes new IEEE-compliant FP64 processing that delivers 2.5x the FP64 performance of V100. </p>
    <figure>
      <img src= "../Images/ampere1.JPG" alt= "Ampere">
     </figure>
     <figure>
    
    <img src= "../Images/ampere2.JPG" alt= "Ampere">
    <figcaption>Comparison of NVIDIA Data Center GPUs [7]</figcaption>
         
    </figure>
    </body>
    
    <body>
       <h2> <a href="https://github.com/GPU-Program-Bug-Study/Comprehensive-Study-on-GPU-Program-Numerical-Issues.github.io/blob/main/NumericalBugDataset.csv" style="text-decoration: underline;">Raw Data</a></h2>
       <iframe src="https://docs.google.com/spreadsheets/d/1uoQYN0usI7R8dIklzo5cggkeCwVm_xkKtWojHCo_jIA/edit?usp=sharingpubhtml?widget=true&amp;headers=false" loading allowfullscreen width="100%" height="300"></iframe>
    </body>
<body>
  <section>
    <section id="Tool">
    <h1>GPU-NBDetect </h1>


    
       <h2> <a href="https://github.com/GPU-Program-Bug-Study/Comprehensive-Study-on-GPU-Program-Numerical-Issues.github.io/tree/main/GPU-NBDetect" style="text-decoration: underline;">Link to GPU-NBDetect</a></h2>
       
    

     
            
        <figure> 
          <img src="./Images/EDE304CF-CE51-4BA0-A635-268AAA689484.jpeg" alt="Overview" width="650" height="400"> 
           <figcaption>Figure 1. Overview on Detecting Numerical Bugs in GPU programs</figcaption>
        </figure>

      <h2>
        Overview of GPU-NBDetect
      </h2>

      <p>
        Our tool, GPU-NBDetect is designed from the findings made in our study. 
        Table 1 in our paper summarizes these findings. 
        The process of numerical bug detection in GPU-NBDetect begins from test input generation (F-7 and I-7 in Table 1)  followed by differential testing with CPU programs (F-8 and I-8 in Table 1), and applying varied manifestation strategies to trigger the bug (F-9, F-10, I-9, and I-10 in Table 1) based on the observed debugging techniques by GPU developers (F 11-14 and I 11-14 in Table 1).  
        Next, we will highlight each component in GPU-NBDetect. 
      </p>

      <p>
        GPU-NBDetect uses a template-based approach to <b>generate both GPU and CPU programs</b>. 
        We leverage Python to dynamically generate C and CUDA files. 
        For CPU program generation, the template reads an operator from a predefined operator list and integrates it into the main function. 
        It also defines the input variables as arguments and includes necessary header files, such as math.h and floating-point exception commands. 
        For GPU program generation, the template handles both the host and device code. 
        The host code manages memory allocation, memory copying, and kernel invocation for the device-side operations. 
        The necessary mathematical operators are directly inserted into the device code from the operator list. Once the CPU and GPU program templates are generated, we compile them using gcc for the CPU code and nvcc for the GPU code to produce the respective object files. 
        The compiled programs are then executed using Python's subprocess command.

      </p>

      <p>
        The process of GPU-NBDetect begins with <b>user-defined inputs</b>, which are typically provided by GPU programmers or users who rely on GPU applications. In our case, we refer to the <a href="https://docs.nvidia.com/cuda/cuda-math-api/index.html" style="text-decoration: underline;">NVIDIA Math Library </a> to ensure the inputs are within the supported range and select appropriate values based on each mathematical operator.

      </p>

      <p>During our study, we observed that developers often test GPU programs with random inputs to identify bugs. 
        For example, to detect rounding errors, GPU developers might experiment with values like 0.5, 0.499999, or 0.50000001. 
        To automate this process and make it more efficient, we developed seven mutation strategies inspired by existing fuzzing techniques, offering a more systematic and affordable approach to generating test inputs. 
        To minimize <b>false positives</b>, each input is subjected to two constraints: 1) verifying that the inputs fall within the supported range for floating-point numbers, and 2) ensuring that the inputs do not overlap with known bug-triggering inputs as identified in the NVIDIA Math Library documentation. 
      </p>

      <ul> 
        <li> bitflip_s: Flips the sign component of a floating-point number with a 50% probability, switching between 0 and 1.</li>
        <li>bitflip_e: Flips a single bit at a random position within the exponent</li>
        <li>bitflip_m: Flips a single bit at a random position within the mantissa</li>
        <li> add_e: Adds a randomly generated integer (between 1 and 32) to the exponent.</li>
        <li>add_m: Adds an integer 2^p ‚àí 1 to the mantissa, where p is randomly generated between 1 and 33.</li>
        <li> sub_e: Subtracts a randomly generated integer (between 1 and 32) from the exponent.</li>
        <li> sub_m: Subtracts an integer 2^p‚àí1 from the mantissa,, where p is randomly generated between 1 and 33.</li>
        <li> random bit: Sets a randomly chosen bit in the exponent or mantissa to a random value</li>
      </ul>

      <p>
        Next, we implemented the manifestation strategies outlined in Table IV, which were inspired by our findings from the study (Table 1 in our paper). 
        To determine the appropriate thresholds for assertions in each manifestation strategy, we tested multiple error bounds to find the most effective range. 
        Bug samples that could not be consistently detected within a specific error bound were excluded, which helped reduce false positives. The selected thresholds strike a balance between accurately detecting bugs and maintaining meaningful comparisons between expected and actual results.

      </p>

      <p>
        From our findings, we observed that GPU programmers frequently compare the results of GPU programs with their CPU counterparts to confirm the presence of a bug. 
        This observation led us to implement differential testing in GPU-NBDetect to initially verify whether GPU programs exhibit numerical errors. 
        Once a bug is detected, we use assertions and manifestation strategies to further classify the type of bug involved.
      </p>
        
        <p>
          Currently, GPU-NBDetect is capable of detecting overflow, underflow, rounding, special value bugs, as well as casting and data type bugs. 
          For each mathematical function, GPU-NBDetect generates 100 input samples to evaluate its effectiveness in identifying potential GPU numerical bugs. 
          Given the inherent randomness of the mutation operators, we conducted three independent runs to ensure the accuracy and consistency of the results.

      </p>

      <h3>Eliminating False-positives: </h3>
      <p>GPU-NBDetect identified 81 bugs, of which 57 were confirmed by developers. 
        As detailed in Table IV, manifestation strategies one through six are designed to reliably detect numerical bugs. 
        To further reduce false positives, we implemented specific constraints during the input generation process: 1) verifying that the inputs are within the supported range for floating-point numbers, and 2) ensuring that these inputs do not overlap with known bug-triggering inputs listed in the NVIDIA Math Library documentation.
      </p>

      <p>
        However, for strategies seven through nine, false positives may still occur. 
        This is due to the reliance on profiling mechanisms, which require both detailed profilers and a wide range of inputs to accurately determine the presence of a bug. 
        Nonetheless, we have minimized the occurrence of false positives by carefully refining the input generation process and applying these constraints.
      </p>

      <h3>Confirmation with developers: </h3>
      <p>
      We reported the detected bugs on the NVIDIA developer forum, where NVIDIA developers could confirm or provide insights regarding the results. 
      Our testing was conducted using the CUDA Math library. 
      In some cases, the <a href="https://docs.nvidia.com/cuda/cuda-math-api/index.html" style="text-decoration: underline;">NVIDIA Math Library </a>  indicated that certain inputs might produce errors.
      However, it was unclear whether these errors fell within the supported range of inputs. 
      GPU-NBDetect successfully identified such numerical bugs, and we reported them to NVIDIA. 
      In response, developers write unit tests to confirm the presence of these bugs. 
      Additionally, developers acknowledged that GPUs lack a built-in mechanism to detect exceptions, and our tool identified exception-triggering inputs that were within the valid range of supported inputs.
      </p>

      <p>
        <b>Fixing these identified numerical bugs</b> presents a challenge due to the nature of GPU architecture. 
        Developers would need to implement floating-point exception flags, rounding registers, and conditional statements in the GPU code to address these issues. 
        Although these measures can resolve numerical bugs, they may also introduce performance-related problems, as GPUs are optimized for parallel computations. 
        For example, addressing overflow, underflow, and special value bugs requires developing exception-handling mechanisms, which can be computationally expensive and may negatively affect performance.
      </p>
        
        <p>
          Given the complexity and performance trade-offs, it is often more practical to rely on third-party tools to detect such bugs. 
          Our tool, GPU-NBDetect, provides a valuable starting point for identifying and addressing these issues without compromising the efficiency of GPU computations.

      </p>

      <h3>
        Comparison with other tools:

      </h3>

      <p>
        While existing tools primarily focus on detecting overflow, underflow, and special value bugs, GPU-NBDetect is designed to detect a broader range of bug types. 
        Furthermore, these tools typically rely on user-defined inputs and lack an automated test input generation strategy, which is a key feature of GPU-NBDetect. 
        This limitation makes it more challenging to reliably identify bug-triggering inputs with those tools.

      </p>

      <p>
        Additionally, a direct comparison between GPU-NBDetect and other tools may not be entirely fair, as GPU-NBDetect emphasizes both bug detection and input generation techniques. 
        However, incorporating the input-generation capabilities of GPU-NBDetect into existing tools could greatly enhance their effectiveness, highlighting another significant contribution our tool offers to the community.
      </p>

      <h3>Examples: </h3> 

      <p>Next, we will highlight some of the interesting examples observed by GPU-NBDetect.</p>

      <ul>
        <li>The cosh function computes calculates the hyperbolic cosine of the input ùë•. This is an increasing function, which is expected to overflow as the input increases. However, <a href="https://docs.nvidia.com/cuda/cuda-math-api/index.html" style="text-decoration: underline;">CUDA Math Library </a> does not specify the range of input values that causes the results to be overflow. 
            However, GPU-NBDetect finds that input value 3.6864e+4 as an input that causes an overflow error. This provides users with more information than the documentation regarding exception inducing inputs. 
            The result is validated with the CPU version too. 
        </li>
        <li>
          The erfc function computes the complementary error function of the input ùë•. According to the <a href="https://docs.nvidia.com/cuda/cuda-math-api/index.html" style="text-decoration: underline;">CUDA Math Library </a>, the function should produce return 2 and +0 for inputs -INF and +INF respectively. 
          However, GPU-NBDetect identifies an underflow error when the input is 6.7076092e+7. 
          This is due to the input being too large, which causes the function to result in a value that is too close to zero that is too small to be represented as a floating point number. 
          We verify the results with the CPU version. 
          Therefore, GPU-NBDetect is capable of identifying such underflow bugs that can provide users with more information regarding the input values that should be used in the computations. 

        </li>

        <li>
          The casting operator __float2int_rd converts a floating point number to an integer by rounding the value towards negative infinity according to the <a href="https://docs.nvidia.com/cuda/cuda-math-api/index.html" style="text-decoration: underline;">CUDA Math Library </a>. 
          For example, the function outputs 24 if the input is 23.5. However, GPU-NBDetect identifies a casting error for an input 1.101256458240e+11 that produces 2.147483648e+9 as the output. 
          The result is the maximum value representable by a 32-bit signed integer. This suggests that the __float2int_rd does not contain enough precision to obtain the result, thus producing the maximum possible value without invoking any exception. GPU-NBDetect has been successful in detecting such numerical bugs in GPU programs.

        </li>

      </ul>


      






  </section>
  </body>
  
    
    <body>
      <h2> Script to run in GitHub Archive </p>
      <p style="color:black;font-size:20px;"> SELECT DISTINCT issue_html, issue_title FROM (
                  SELECT type, repo.name as repo, <br>
                  JSON_EXTRACT(payload, '$.issue.html_url') as issue_html, <br>
                  JSON_EXTRACT(payload, '$.issue.title') as issue_title,  <br>
                  JSON_EXTRACT(payload, '$.issue.body') as issue_desc,    <br>
                  JSON_EXTRACT(payload, '$.issue.state') as issue_state   <br>
                  FROM `githubarchive.year.2019`<br>
                  WHERE type = 'IssuesEvent'
                   ) <br>
                  WHERE ((lower(issue_title) LIKE '%GPU%' or lower(issue_desc) LIKE '%GPU%') OR (lower(issue_title) LIKE '%GPU Programming%' or lower(issue_desc) LIKE '%GPU Programming%'))
        ORDER BY issue_html DESC; <br> </p>

      
      
   </body>
    <body> 
        <h2> References: </h2>
        <p>
        1. Dally, William J., et al. ‚ÄúEvolution of the Graphics Processing Unit (GPU).‚Äù IEEE Micro, vol. 41, no. 6, 2021, pp. 42‚Äì51., doi:10.1109/mm.2021.3113475. <br> 
        2. Nvidia Fermi Architecture Whitepaper. <a href="www.nvidia.com/content/PDF/fermi_white_papers/NVIDIAFermiComputeArchitectureWhitepaper.pdf">www.nvidia.com/content/PDF/fermi_white_papers/NVIDIAFermiComputeArchitectureWhitepaper.pdf. </a> <br>
        3. CUDATM Compute Architecture: Kepler TM GK110/210 - Nvidia. <a href= "www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/tesla-product-literature/NVIDIA-Kepler-GK110-GK210-Architecture-Whitepaper.pdf">www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/tesla-product-literature/NVIDIA-Kepler-GK110-GK210-Architecture-Whitepaper.pdf </a>. <br>
        4. Nvidia Fermi Architecture Whitepaper. <a href= "www.nvidia.com/content/PDF/fermi_white_papers/NVIDIAFermiComputeArchitectureWhitepaper.pdf">www.nvidia.com/content/PDF/fermi_white_papers/NVIDIAFermiComputeArchitectureWhitepaper.pdf </a> <br> 
        5. Volta Architecture - Nvidia. <a href="images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf> images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf </a>. <br>
        6. Nvidia <a href="images.nvidia.com/aem-dam/en-zz/Solutions/design-visualization/technologies/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf">images.nvidia.com/aem-dam/en-zz/Solutions/design-visualization/technologies/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf </a>. <br> 
        7. Nvidia A100 Tensor Core GPU Architecture <a href="images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf">images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf</a>. <br>
            </p>

    </body>
    
</html>  
