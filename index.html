<!DOCTYPE html>
<html>
    <head>
        <h1>A Comprehensive Study on Numerical Issues in GPU Programs</h1>
    </head>
    
<h2 id="tables">Tables</h2>
<p id="tableA">Table A shows the total GitHub repositories included in the study after performing the manual inspection</p>
<label   for="tableA">Table A: Summary of GitHub Issues</label>
<table  class="table table-bordered table-hover table-condensed" name="tableA">
    <thead><tr><th title="Field #1">GitHub Project</th>
    <th title="Field #2">Stars</th>
    <th title="Field #3">Commits</th>
    <th title="Field #4">Contributors</th>
    <th title="Field #5">Issues Included</th>
    </tr></thead>
    <tbody><tr>
    <td>CuPy</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">62</td>
    </tr>
    <tr>
    <td>TensorFlow</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">42</td>
    </tr>
    <tr>
    <td>PyTorch</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">30</td>
    </tr>
    <tr>
    <td>cuDF</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">15</td>
    </tr>
    <tr>
    <td>Numba</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">14</td>
    </tr>
    <tr>
    <td>Tranformers</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">10</td>
    </tr>
    <tr>
    <tr>
    <td>Gingko</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">5</td>
    </tr>
    <tr>
    <td>cuML</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">5</td>
    </tr>
    <tr>
    <td>TensorRT</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">5</td>
    </tr>
    <tr>
    <td>Apache</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">4</td>
    </tr>
    <tr>
    <td>mmdetection</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">4</td>
    </tr>
    <tr>
    <td>PyTorch-Lightning</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">4</td>
    </tr>
    <tr>
    <td>MatX</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">4</td>
    </tr>
    <tr>
    <td>Paddle</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">3</td>
    </tr>
    <tr>
    <td>Cutlass</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">3</td>
    </tr>
    <tr>
    <td>GPUweb</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    <td align="right">3</td>
    </tr>
    <tr>
    <td>Onnx</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">3</td>
    </tr>
    <tr>
    <td>xgboost</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">3</td>
    </tr>
    <tr>
    <td>pyVista</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">2</td>
    </tr>
    <tr>
    <td>cuGraph</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">2</td>
    </tr>
    <tr>
    <td>CUDA.jl</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">2</td>
    </tr>
    <tr>
    <td>pymc-dev</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">2</td>
    </tr>
    <tr>
    <td>google</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">2</td>
    </tr>
    <tr>
    <td>CLIMA</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">2</td>
    </tr>
    <tr>
    <td>ray</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>napari</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>jiesutdNCRF</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
    <tr>
    <td>AMDMIGraphX</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>Thrust</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>GPytorch</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>Stumpy</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>Spark-rapids</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>UPIT</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>DeepSpeed</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>CSAROpen</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>OpenCV</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>catboot</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>NX</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>Onnxruntime</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>mrdoob</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>AMDLk</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>WGPU</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>FARM</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>hoomd-blue</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>blazingsql</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>awslabs djl</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>MIOpen</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>Kitty</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
     <tr>
    <td>faiss</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>Chaim</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>GPFlow</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
     <tr>
    <td>OpenNMT</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>gslang</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>maskrcnn-benchmark</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
    <tr>
    <td>futhark</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>Meinsum.jl</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>facebookresearch</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
     <tr>
    <td>uber</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>tensorpack</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>spaCy</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>perses</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>linux</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>verification-classifier</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
   <tr>
    <td>pointcloud</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
    <tr>
    <td>darknet</td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right"></td>
    <td align="right">1</td>
    </tr>
</tbody></table>
<br/>
<br/>
    
    
    
    <body> 
        <head>
           <h2> Floating Point Support in GPU Architecture </h2>
        </head>
        
        <p style="color:black;font-size:20px;"> GeForce 256 was the first chip to combine the vertex computations for transformation and lighting and the fragment computations on the same chip[1]. 
             This fully integrated graphics engine was named a “graphics processing unit” or GPU. 
             Offloading the vertex computations from the host enabled higher geometric complexity in games at the cost of requiring significant floating-point performance.
             For example, the perspective transformation requires a 4X4 matrix–vector multiply and a perspective division operation. </p>
        
        <p style="color:black;font-size:20px;"> GeForce 6 had a peak performance of 108 billion single-precision floating-point operations per second (108 GFLOPS) [1]. 
               However, programming these GPUs was challenging because the input and output were very restricted. 
               Fragment shaders only received input from interpolated vertex attributes and textures and only deposited output into the frame buffer. 
               Programmers had to cast their applications as rendering texture mapped and blended triangles to harness the FLOPS of these GPUs[1]. </p>
        
        <p style="color:black;font-size:20px;"> The stream processing model influenced the design of GPUs intended for computing [1]. 
               The GeForce 8 GPU or G80 introduced streaming multiprocessors (SMs) that were used to run both vertex and fragment shaders.
               These SMs could also run “compute shaders” independent of the graphics pipeline specifically in the form of CUDA cooperative thread arrays. 
               A shared memory facilitated communication between the threads in an array. 
               The G80 “Tesla” GPU and CUDA reduced the barriers to developing GPU-based scientific and other general-purpose applications[1]. 
               G80 was a massively parallel threaded processor, and CUDA was a thread- and data-parallel C-like programming language. </p>
        
        <p style="color:black;font-size:20px;"> The development of Fermi generation of GPUs helps to address the floating point issues in GPUs [1][2]. 
         Single precision floating point instructions  support subnormal numbers by default in hardware. 
         Subnormal numbers are small numbers that lie between zero and the smallest normalized number of a given floating point number system.
         Fermi’s floating point units handle subnormal numbers in hardware, allowing values to gradually underflow to zero with no performance penalty. 
         Additionally, Fermi GPUs add support to double precision computation. </p>
          
    <figure>
    <img src= "../Images/fermi.JPG" alt= "Fermi">
    <figcaption>Comparison of Fermi with previous GPUs[2]</figcaption>
    </figure>
        
         
        <p style="color:black;font-size:20px;"> With the development of Kepler GPUs, the support for atomic memory operations helped parallel programming to gain massive performance benefits[3]. 
            Throughput of global memory atomic operations on Kepler GK110/210 are substantially improved compared to the Fermi generation. 
            Atomic operation throughput to a common global memory address is improved by 9x to one operation per clock. 
            Atomic operation throughput to independent global addresses is also significantly accelerated, and logic to handle address conflicts has been made more efficient. 
            Atomic operations can often be processed at rates similar to global load operations. 
             This speed increase makes atomics fast enough to use frequently within kernel inner loops, eliminating the separate reduction passes that were previously required by some algorithms to consolidate results. 
             Kepler GK110 also expands the native support for 64-bit atomic operations in global memory. 
             In addition to atomicAdd, atomicCAS, and atomicExch (which were also supported by Fermi and Kepler GK104), GK110 supports the following:  atomicMin,  atomicMax,  atomicAnd,  atomicOr,  atomicXor.
            Other atomic operations which are not supported natively (for example 64-bit floating point atomics) may be emulated using the compare-and-swap (CAS) instruction. </p>
        
        <p style="color:black;font-size:20px;"> According to Dally et al. [1], the next level of support is a set of numerical libraries including CuBLAS, CuSparse, and CuFFT that provide highly optimized code for the key functions of many numerical programs. 
         Applications can then leverage these libraries to exploit GPU capabilities. 
         Today, over 600 HPC applications are accelerated by GPUs,9 including molecular dynamics codes such as GROMACS, NAMD, AMBER, and LAMMPS; weather codes such as WRF; fluid dynamics codes such as ANSYS and OpenFOAM; 
        chemistry codes such as Gaussian, VASP, Quantum Espresso, and GAMESS; and structural analysis codes such as LS-DYNA and ANSYS. </p>


       <p style="color:black;font-size:20px;"> Pascal GPUs  provide superior scheduling and overlapped load/store instructions to increase floating point utilization[4]. 
        New innovations in our Pascal architecture, including native 16-bit floating point (FP) precision, allow GP100 to deliver great speedups for many Deep Learning algorithms. 
        These algorithms do not require high levels of floating-point precision, but they gain large benefits from the additional computational power FP16 affords, and the reduced storage requirements for 16-bit datatypes. 
        Storing FP16 data compared to higher precision FP32 or FP64 reduces memory usage of the neural network and thus allows training and deploying of larger networks. 
        Using FP16 computation improves performance up to 2x compared to FP32 arithmetic, and similarly FP16 data transfers take less time than FP32 or FP64 transfers.
        Also, The atomic addition operation in global memory has been extended to include FP64 data. 
        The atomicAdd() function in CUDA now applies to 32 and 64-bit integer and floating-point data. 
        The rounding mode for floating-point is round-to-nearest-even for all floating-point atomic add operations (formerly, FP32 atomic addition used round-to-zero).</p>
   
     <figure>
    <img src= "../Images/pascal.JPG" alt= "Pascal">
    <figcaption>Tesla P100 Compared to Prior Generation Tesla products [4]</figcaption>
    </figure>
          
      <p style="color:black;font-size:20px;">  Tesla V100 GPUs deliver industry-leading floating-point and integer performance [5].
       The Tesla V100 GPU contains 640 Tensor Cores: eight (8) per SM and two (2) per each processing block (partition) within an SM. 
      In Volta GV100, each Tensor Core performs 64 floating point FMA operations per clock, and eight Tensor Cores in an SM perform a total of 512 FMA operations (or 1024 individual floating point operations) per clock.
      For deep learning inference, V100 Tensor Cores provide up to 6x higher peak TFLOPS compared to standard FP16 operations on P100. 
      Tensor Cores and their associated data paths are custom-designed to dramatically increase floating-point compute throughput with high energy efficiency. </p>
      
    <figure>
    <img src= "../Images/volta.JPG" alt= "Volta">
    <figcaption>Comparison of NVIDIA Tesla GPUs [5]</figcaption>
    </figure>
        
      <p style="color:black;font-size:20px;"> A Turing 102 GPU contains 576 Tensor Cores: eight per SM and two per each processing block within an SM [6]. 
      Each Tensor Core can perform up to 64 floating point fused multiply-add (FMA) operations per clock using FP16 inputs. 
      Eight Tensor Cores in an SM perform a total of 512 FP16 multiply and accumulate operations per clock, or 1024 total FP operations per clock. 
      The new INT8 precision mode works at double this rate, or 2048 integer operations per clock. 
       Turing Tensor Cores provide significant speedups to matrix operations and are used for both deep learning training and inference operations in addition to new neural graphics functions. </p>
     
      <figure>
      <img src= "../Images/Turing1.JPG" alt= "Turing">
      </figure>
        
     <figure>
    <img src= "../Images/Turing2.JPG" alt= "Turing">
    <figcaption>Comparison of NVIDIA Pascal GP104 to Turing TU106 GPUs [6]</figcaption>
    </figure>
        
       <p style="color:black;font-size:20px;"> Ampere GPUs provide TensorFloat support to GPU architecture [1] [7]. 
       New TensorFloat-32 (TF32) Tensor Core operations in A100 provide an easy path to accelerate FP32 input/output data in DL frameworks and HPC, running 10x faster than V100 FP32 FMA operations or 20x faster with sparsity.  
       The Tensor Core architecture also supports high-throughput computation for different numerical representations, including binary (INT1), INT4, INT8, FP16, and BFloat16 (8-bit exponent, 7-bit mantissa). 
       For FP16/FP32 mixed-precision DL, the A100 Tensor Core delivers 2.5x the performance of V100, increasing to 5x with sparsity. 
       New Bfloat16 (BF16)/FP32 mixed-precision Tensor Core operations run at the same rate as FP16/FP32 mixed-precision. 
       Tensor Core acceleration of INT8, INT4, and binary round out support for DL inferencing, with A100 sparse INT8 running 20x faster than V100 INT8. 
       For HPC, the A100 Tensor Core includes new IEEE-compliant FP64 processing that delivers 2.5x the FP64 performance of V100. </p>
    <figure>
      <img src= "../Images/ampere1.JPG" alt= "Ampere">
     </figure>
     <figure>
    
    <img src= "../Images/ampere2.JPG" alt= "Ampere">
    <figcaption>Comparison of NVIDIA Data Center GPUs [7]</figcaption>
         
    </figure>
    </body>
    
    <body>
       <h2> <a href="https://github.com/GPU-Program-Bug-Study/Comprehensive-Study-on-GPU-Program-Numerical-Issues.github.io/blob/main/AStudyonNumericalBugs.csv" style="text-decoration: underline;">Raw Data</a></h2>
       <iframe src="https://docs.google.com/spreadsheets/d/1OswKlInm_h6FnTnjfCuYJpcwigkuDnIeAYh3ffr5lgg/edit?usp=sharing/pubhtml?widget=true&amp;headers=false" loading allowfullscreen width="100%" height="300"></iframe>
    </body>
    
    <body> 
        <h2> References: </h2>
        <p>
        1. Dally, William J., et al. “Evolution of the Graphics Processing Unit (GPU).” IEEE Micro, vol. 41, no. 6, 2021, pp. 42–51., doi:10.1109/mm.2021.3113475. <br> 
        2. Nvidia Fermi Architecture Whitepaper. <a href="www.nvidia.com/content/PDF/fermi_white_papers/NVIDIAFermiComputeArchitectureWhitepaper.pdf">www.nvidia.com/content/PDF/fermi_white_papers/NVIDIAFermiComputeArchitectureWhitepaper.pdf. </a> <br>
        3. CUDATM Compute Architecture: Kepler TM GK110/210 - Nvidia. <a href= "www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/tesla-product-literature/NVIDIA-Kepler-GK110-GK210-Architecture-Whitepaper.pdf">www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/tesla-product-literature/NVIDIA-Kepler-GK110-GK210-Architecture-Whitepaper.pdf </a>. <br>
        4. Nvidia Fermi Architecture Whitepaper. <a href= "www.nvidia.com/content/PDF/fermi_white_papers/NVIDIAFermiComputeArchitectureWhitepaper.pdf">www.nvidia.com/content/PDF/fermi_white_papers/NVIDIAFermiComputeArchitectureWhitepaper.pdf </a> <br> 
        5. Volta Architecture - Nvidia. <a href="images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf> images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf </a>. <br>
        6. Nvidia <a href="images.nvidia.com/aem-dam/en-zz/Solutions/design-visualization/technologies/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf">images.nvidia.com/aem-dam/en-zz/Solutions/design-visualization/technologies/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf </a>. <br> 
        7. Nvidia A100 Tensor Core GPU Architecture <a href="images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf">images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf</a>. <br>
            </p>

    </body>
    
</html>  
